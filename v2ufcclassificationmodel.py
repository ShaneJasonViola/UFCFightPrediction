# -*- coding: utf-8 -*-
"""V2ufcclassificationmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ff0N5DihJPiv0r0nMxDMuIvnfbtw0YwW

Imports
"""

# 1. Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from scipy.stats import mode
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, cross_val_predict, StratifiedKFold
from sklearn.base import clone
import joblib



# Style settings
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette('husl')

"""Load Data"""

# 2. Load Data

# Load UFC dataset from uploaded CSV in Colab


# Load from local path in Colab
file_path = '/content/ufc-master.csv'
ufc = pd.read_csv(file_path)

# Create a working copy for processing
df = ufc.copy()

"""Show first 5 Rows"""

# Show first 5 rows
df.head()

"""Basic Information about the dataset"""

# Ensure all columns are printed in full
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)

# Create summary of data types and missing values
column_summary = pd.DataFrame({
    'DataType': df.dtypes,
    'MissingValues': df.isnull().sum(),
    'MissingPercent': (df.isnull().sum() / len(df)) * 100  # Optional: % missing
})

# Sort by missing values
column_summary = column_summary.sort_values(by='MissingValues', ascending=False)

# Display full summary
display(column_summary)

"""Filter out Unnecessary Columns"""

# Base columns you want to keep for modeling and feature engineering
columns_to_keep = [
    'Winner',
    'RedWins', 'RedLosses', 'BlueWins', 'BlueLosses',
    'RedWinsByKO', 'BlueWinsByKO', 'RedWinsBySubmission', 'BlueWinsBySubmission',
    'RedAvgSigStrPct', 'BlueAvgSigStrPct',
    'RedAvgTDLanded', 'BlueAvgTDLanded',
    'RedAvgTDPct', 'BlueAvgTDPct',
    'RedAvgSubAtt', 'BlueAvgSubAtt',
    'RedAvgSigStrLanded', 'BlueAvgSigStrLanded',
    'RedReachCms', 'BlueReachCms',
    'RedHeightCms', 'BlueHeightCms',
    'RedAge', 'BlueAge', 'HeightDif',
    'RedCurrentLoseStreak', 'BlueCurrentLoseStreak',
    'RedCurrentWinStreak', 'BlueCurrentWinStreak',


]

# Identify missing columns
missing_columns = [col for col in columns_to_keep if col not in df.columns]
if missing_columns:
    print("The following columns are missing from the dataset:")
    print(missing_columns)

# Only keep columns that exist in the DataFrame
df = df.loc[:, df.columns.intersection(columns_to_keep)].copy()

# Optional: show what columns you successfully kept
print(f"Remaining columns in dataset: {df.columns.tolist()}")

"""Calculate New Features"""

# Win/Loss Ratio
if all(col in df.columns for col in ['RedWins', 'RedLosses']):
    df['RedWinLossRatio'] = df['RedWins'] / (df['RedWins'] + df['RedLosses'] + 1)

if all(col in df.columns for col in ['BlueWins', 'BlueLosses']):
    df['BlueWinLossRatio'] = df['BlueWins'] / (df['BlueWins'] + df['BlueLosses'] + 1)

df['WinLossRatioDif'] = df.get('RedWinLossRatio', 0) - df.get('BlueWinLossRatio', 0)


# Submission %
if all(col in df.columns for col in ['RedWinsBySubmission', 'RedWins']):
    df['RedSubPct'] = df['RedWinsBySubmission'] / (df['RedWins'] + 1)

if all(col in df.columns for col in ['BlueWinsBySubmission', 'BlueWins']):
    df['BlueSubPct'] = df['BlueWinsBySubmission'] / (df['BlueWins'] + 1)

df['SubPctDiff'] = df.get('RedSubPct', 0) - df.get('BlueSubPct', 0)

# Fight Experience
if all(col in df.columns for col in ['RedWins', 'RedLosses']):
    df['RedTotalFights'] = df['RedWins'] + df['RedLosses']

if all(col in df.columns for col in ['BlueWins', 'BlueLosses']):
    df['BlueTotalFights'] = df['BlueWins'] + df['BlueLosses']

df['FightExperienceDiff'] = df.get('RedTotalFights', 0) - df.get('BlueTotalFights', 0)

# Stat Differences
if all(col in df.columns for col in ['RedAvgTDLanded', 'BlueAvgTDLanded']):
    df['TDLandedDif'] = df['RedAvgTDLanded'] - df['BlueAvgTDLanded']

if all(col in df.columns for col in ['RedAvgSigStrPct', 'BlueAvgSigStrPct']):
    df['SigStrPctDif'] = df['RedAvgSigStrPct'] - df['BlueAvgSigStrPct']

if all(col in df.columns for col in ['RedReachCms', 'BlueReachCms']):
    df['ReachDif'] = df['RedReachCms'] - df['BlueReachCms']

if all(col in df.columns for col in ['RedAge', 'BlueAge']):
    df['AgeDif'] = df['RedAge'] - df['BlueAge']

if all(col in df.columns for col in ['RedAvgTDPct', 'BlueAvgTDPct']):
    df['TDPctDiff'] = df['RedAvgTDPct'] - df['BlueAvgTDPct']

if all(col in df.columns for col in ['RedAvgSubAtt', 'BlueAvgSubAtt']):
    df['SubAttDif'] = df['RedAvgSubAtt'] - df['BlueAvgSubAtt']

if all(col in df.columns for col in ['RedAvgSigStrLanded', 'BlueAvgSigStrLanded']):
    df['SigStrLandedDif'] = df['RedAvgSigStrLanded'] - df['BlueAvgSigStrLanded']

df.head()

"""Flip Winning Ratio so amount of wins is balanced for Red & Blue"""

# ---------------------------------------------------------
# Winner column handling and Red/Blue flip logic
# ---------------------------------------------------------

# Step 1: Normalize column names in both the original dataset (ufc) and working DataFrame (df)
# This removes leading/trailing spaces and makes column name matching consistent.
ufc.columns = ufc.columns.str.strip()
df.columns = df.columns.str.strip()

# Step 2: Identify the 'Winner' column from the original CSV.
# First, check for an exact match (case-insensitive) to "winner".
winner_candidates_exact = [c for c in ufc.columns if c.strip().lower() == 'winner']

if len(winner_candidates_exact) > 0:
    winner_col = winner_candidates_exact[0]
else:
    # If there is no exact match, look for any column that contains the word "winner".
    winner_candidates_fuzzy = [c for c in ufc.columns if 'winner' in c.strip().lower()]
    if len(winner_candidates_fuzzy) > 0:
        winner_col = winner_candidates_fuzzy[0]
    else:
        # If no match is found, raise an error and display all columns for troubleshooting.
        raise KeyError(
            "Could not find a 'Winner' column in your CSV. "
            "Check the CSV headers: "
            f"{list(ufc.columns)}"
        )

# Step 3: Ensure the 'Winner' column exists in df.
# If it was dropped earlier, merge it back from the original 'ufc' dataset.
if 'Winner' not in df.columns:
    # Bring in the Winner column from the original data, aligning by index.
    df = df.merge(ufc[[winner_col]], left_index=True, right_index=True, how='left')
    if winner_col != 'Winner':
        # Rename the column to exactly 'Winner' for consistency.
        df.rename(columns={winner_col: 'Winner'}, inplace=True)
else:
    # If the Winner column exists but has a different name, rename it to 'Winner'.
    if winner_col != 'Winner':
        df.rename(columns={winner_col: 'Winner'}, inplace=True)

# Step 4: Filter out rows where 'Winner' is not 'Red' or 'Blue'.
# Convert to lowercase for case-insensitive comparison.
df['Winner'] = df['Winner'].astype(str).str.strip()
valid_mask = df['Winner'].str.lower().isin(['red', 'blue'])
df = df.loc[valid_mask].copy()

# Step 5: Create a binary winner column:
# 1 = Red Fighter wins, 0 = Blue Fighter wins.
df['WinnerBinary'] = (df['Winner'].str.lower() == 'red').astype(int)

# Step 6: Identify all columns that belong to the Red or Blue fighter.
# This will be used for flipping to make the model color-agnostic.
red_cols = [col for col in df.columns if col.startswith("Red")]
blue_cols = [col for col in df.columns if col.startswith("Blue")]

# Step 7: Create a random mask to select 50% of rows to flip.
# This random flipping removes color bias from the model.
np.random.seed(42)
flip_mask = np.random.rand(len(df)) < 0.5

# Step 8: Flip the Red and Blue fighter stats for the selected rows.
for red_col in red_cols:
    blue_col = red_col.replace("Red", "Blue")
    if blue_col in df.columns:
        temp = df.loc[flip_mask, red_col].copy()
        df.loc[flip_mask, red_col] = df.loc[flip_mask, blue_col]
        df.loc[flip_mask, blue_col] = temp

# Step 9: Flip the WinnerBinary label for the flipped rows.
df.loc[flip_mask, 'WinnerBinary'] = 1 - df.loc[flip_mask, 'WinnerBinary']

# Step 10: (Optional) Drop the text-based 'Winner' column if only the binary is needed.
# df.drop(columns=['Winner'], inplace=True)

# Step 11: Print the class balance for verification.
print("Class balance (0 = Blue win, 1 = Red win):")
print(df['WinnerBinary'].value_counts(dropna=False))

df.head()

# List of all relevant features including target
features = [
    'RedWinLossRatio', 'BlueWinLossRatio', 'RedWins', 'BlueWins',
    'RedAge', 'BlueAge', 'AgeDif',
    'RedAvgTDLanded', 'BlueAvgTDLanded', 'RedAvgTDPct', 'BlueAvgTDPct',  'TDPctDiff',
    'RedAvgSigStrPct', 'BlueAvgSigStrPct', 'SigStrPctDif',
    'RedAvgSigStrLanded', 'BlueAvgSigStrLanded', 'SigStrLandedDif',
    'SubAttDif', 'WinnerBinary', 'HeightDif',
    'ReachDif', 'FightExperienceDiff',
    'RedCurrentWinStreak', 'BlueCurrentWinStreak',
    'RedCurrentLoseStreak', 'BlueCurrentLoseStreak',

]

# Compute correlations with WinnerBinary
corr_df = df[features].corr()['WinnerBinary'].drop('WinnerBinary').sort_values(ascending=False)

# Print correlation values as a table
print("Correlation of Features with WinnerBinary:\n")
print(corr_df.to_frame(name='Correlation'))

# Plot the sorted correlation values
plt.figure(figsize=(12, 10))
corr_df.plot(kind='barh', color='skyblue')
plt.title('Feature Correlation with WinnerBinary')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Features')
plt.grid(True)
plt.gca().invert_yaxis()  # Highest at top
plt.tight_layout()
plt.show()

# Remove duplicate columns from the heatmap input
clean_features = list(dict.fromkeys(features + ['WinnerBinary']))

plt.figure(figsize=(10, 8))
sns.heatmap(df[clean_features].corr(), annot=True, fmt=".1f", cmap="coolwarm")
plt.title("Correlation Heatmap of Features")
plt.show()

# Total wins by method
ko_wins = df['RedWinsByKO'].sum() + df['BlueWinsByKO'].sum()
sub_wins = df['RedWinsBySubmission'].sum() + df['BlueWinsBySubmission'].sum()

# Assuming total wins = RedWins + BlueWins
total_wins = df['RedWins'].sum() + df['BlueWins'].sum()
decision_wins = total_wins - (ko_wins + sub_wins)

# Create DataFrame for plotting
win_data = pd.DataFrame({
    'Method': ['KO', 'Submission', 'Decision'],
    'Wins': [ko_wins, sub_wins, decision_wins]
})

# Convert to percentages
win_data['Percentage'] = (win_data['Wins'] / win_data['Wins'].sum()) * 100

# Plot
plt.figure(figsize=(8, 6))
bars = plt.bar(win_data['Method'], win_data['Percentage'], color=['crimson', 'darkblue', 'gray'])

# Add labels on top
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, height + 1, f'{height:.1f}%', ha='center', va='bottom', fontsize=12)

plt.ylim(0, 100)
plt.ylabel('Percentage of Total Wins')
plt.title('Win Method Distribution (KO vs Submission vs Decision)')
plt.tight_layout()
plt.show()

df.head()

"""Feature Selection"""

features = [
    'RedWinLossRatio', 'BlueWinLossRatio',  #'RedWins', 'BlueWins',
    'RedAge', 'BlueAge', 'AgeDif',
    'RedAvgTDLanded', 'BlueAvgTDLanded', 'TDLandedDif',
    'RedAvgTDPct', 'BlueAvgTDPct', 'TDPctDiff',
    'RedAvgSigStrPct', 'BlueAvgSigStrPct', 'SigStrPctDif',
    'RedAvgSigStrLanded', 'BlueAvgSigStrLanded', 'SigStrLandedDif',
    'ReachDif', 'SubAttDif', 'HeightDif',
    'RedCurrentWinStreak', 'BlueCurrentWinStreak',
    'RedCurrentLoseStreak', 'BlueCurrentLoseStreak',

]

"""Split Features and Target"""

# Split Features and Target
X = df[features]
y = df['WinnerBinary']

"""Fill in Missing Values"""

imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

"""Feature Scaling"""

# Initialize the scaler
scaler = StandardScaler()
x_scaled = scaler.fit_transform(X_imputed)

"""Train Test Split"""

# Split into train/test sets
x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.3, random_state=42)

"""Train Model"""

from sklearn.base import clone
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.cluster import KMeans
from scipy.stats import mode
import matplotlib.pyplot as plt
import numpy as np

# Define tuned models
models = {
    'LOG-R': LogisticRegression(max_iter=1000),
    'GNB': GaussianNB(),
    'DT': DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=42),
    'RF': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),
    'SVM': SVC(C=0.1, gamma=0.01, kernel='rbf', probability=True),
    'KNN': KNeighborsClassifier(n_neighbors=25, weights='uniform', metric='euclidean')
}

# Initialize metric storage
model_names = []
train_accs, test_accs = [], []
train_prec, test_prec = [], []
train_rec, test_rec = [], []
train_f1s, test_f1s = [], []
classification_reports = {}

# Train and evaluate each model
for name, model in models.items():
    clf = clone(model)
    clf.fit(x_train, y_train)

    # Training predictions
    y_train_pred = clf.predict(x_train)
    acc_train = accuracy_score(y_train, y_train_pred)
    train_accs.append(acc_train)
    train_prec.append(precision_score(y_train, y_train_pred, average='weighted', zero_division=0))
    train_rec.append(recall_score(y_train, y_train_pred, average='weighted', zero_division=0))
    train_f1s.append(f1_score(y_train, y_train_pred, average='weighted', zero_division=0))

    # Testing predictions
    y_test_pred = clf.predict(x_test)
    acc_test = accuracy_score(y_test, y_test_pred)
    test_accs.append(acc_test)
    test_prec.append(precision_score(y_test, y_test_pred, average='weighted', zero_division=0))
    test_rec.append(recall_score(y_test, y_test_pred, average='weighted', zero_division=0))
    test_f1s.append(f1_score(y_test, y_test_pred, average='weighted', zero_division=0))

    model_names.append(name)

    # Save detailed classification reports
    classification_reports[name] = {
        'Train': f"Accuracy: {acc_train:.4f}\n" + classification_report(y_train, y_train_pred, zero_division=0),
        'Test': f"Accuracy: {acc_test:.4f}\n" + classification_report(y_test, y_test_pred, zero_division=0)
    }

# KMeans with optimal k=5
optimal_k = 5
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans.fit(x_train)

cluster_labels = kmeans.predict(x_test)

# Map clusters to majority class labels from training set
train_cluster_labels = kmeans.predict(x_train)
cluster_to_label = {}

for cluster in range(optimal_k):
    indices = np.where(train_cluster_labels == cluster)[0]  # get index positions as array
    if len(indices) > 0:
        majority_label = mode(y_train.iloc[indices], keepdims=False).mode
        cluster_to_label[cluster] = majority_label
    else:
        cluster_to_label[cluster] = 0  # fallback default

# Map predicted clusters to labels
mapped_labels = np.array([cluster_to_label[c] for c in cluster_labels])

# Append results
model_names.append("KMeans_k=5")
train_accs.append(None)
train_prec.append(None)
train_rec.append(None)
train_f1s.append(None)

acc_kmeans = accuracy_score(y_test, mapped_labels)
test_accs.append(acc_kmeans)
test_prec.append(precision_score(y_test, mapped_labels, average='weighted', zero_division=0))
test_rec.append(recall_score(y_test, mapped_labels, average='weighted', zero_division=0))
test_f1s.append(f1_score(y_test, mapped_labels, average='weighted', zero_division=0))

classification_reports['KMeans_k=5'] = {
    'Train': "Not applicable for unsupervised learning",
    'Test': f"Accuracy: {acc_kmeans:.2f}\n" + classification_report(y_test, mapped_labels, zero_division=0)
}

from sklearn.ensemble import VotingClassifier

# Ensemble
ensemble = VotingClassifier(
    estimators=[
        ('SVM', models['SVM']),
        ('RF', models['RF'])
    ],
    voting='soft'
)

# Fit on training data
ensemble.fit(x_train, y_train)

# Evaluate
print("Training accuracy:", ensemble.score(x_train, y_train))
print("Testing accuracy:", ensemble.score(x_test, y_test))

"""Evaluation Metrics"""

# Print all classification reports
for model in model_names:
    print(f"\n====== {model} ======")
    print("Train Report:\n", classification_reports[model]['Train'])
    print("Test Report:\n", classification_reports[model]['Test'])

# Plot test performance metrics
plt.figure(figsize=(12, 6))
x = np.arange(len(model_names))
bar_width = 0.2

plt.bar(x - 1.5*bar_width, test_accs, width=bar_width, label='Accuracy')
plt.bar(x - 0.5*bar_width, test_prec, width=bar_width, label='Precision')
plt.bar(x + 0.5*bar_width, test_rec, width=bar_width, label='Recall')
plt.bar(x + 1.5*bar_width, test_f1s, width=bar_width, label='F1 Score')

plt.xticks(x, model_names)
plt.ylabel('Score')
plt.title('Model Performance on Test Set')
plt.legend()
plt.tight_layout()
plt.show()

"""Cross Validation"""

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_means = []

# Evaluate each model using cross_val_score
for name, model in models.items():
    scores = cross_val_score(model, x_scaled, y, cv=cv, scoring='accuracy')
    mean_score = np.mean(scores)
    print(f"{name} - Fold Accuracies: {scores} | Mean Accuracy: {mean_score:.4f}")
    cv_means.append(mean_score)

# Plot
plt.figure(figsize=(10, 5))
plt.bar(models.keys(), cv_means, color='skyblue')
plt.ylabel("Mean Accuracy")
plt.title("5-Fold Cross-Validation Accuracy for Each Model")
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
for i, v in enumerate(cv_means):
    plt.text(i, v + 0.01, f"{v:.2f}", ha='center', fontsize=10)
plt.tight_layout()
plt.show()

"""Elbow Method for Optimal k"""

# Elbow Method for Optimal k
inertia = []
K = range(1, 15)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    kmeans.fit(x_scaled)
    inertia.append(kmeans.inertia_)

# Plotting the Elbow Curve
plt.figure(figsize=(10, 6))
plt.plot(K, inertia, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum of Squared Distances (Inertia)')
plt.title('Elbow Method For Optimal k')
plt.grid(True)
plt.show()

# Train Random Forest if not already
rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)
rf.fit(x_train, y_train)

# Plot feature importances
importances = rf.feature_importances_
feature_names = df[features].columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances - Random Forest")
plt.bar(range(len(importances)), importances[indices], align="center")
plt.xticks(range(len(importances)), feature_names[indices], rotation=45, ha='right')
plt.ylabel("Importance Score")
plt.tight_layout()
plt.show()

# Train SVM (make sure your x_train/x_test are scaled for SVM)
svm = SVC(probability=True, kernel='rbf', C=1, gamma='scale', random_state=42)
svm.fit(x_train, y_train)

# Predict on test data
y_pred_svm = svm.predict(x_test)

# Create confusion matrix
cm_svm = confusion_matrix(y_test, y_pred_svm)

# Plot heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Loss', 'Win'], yticklabels=['Loss', 'Win'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - SVM')
plt.tight_layout()
plt.show()

"""Dowwnload Model"""

import joblib
from google.colab import files   # <-- this is required for files.download
from sklearn.svm import SVC

# Train SVM (use scaled features for better accuracy)
svm = SVC(probability=True, kernel='rbf', C=1, gamma='scale', random_state=42)
svm.fit(x_train, y_train)

# Save the trained SVM model
joblib.dump(svm, 'svm_model.pkl')

# Download the saved model
files.download('svm_model.pkl')

'''
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# 1. RandomForestClassifier
rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30]
}

rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy', n_jobs=-1)
rf_grid.fit(x_train, y_train)

print("Best Random Forest Params:", rf_grid.best_params_)
print("Best Random Forest Score:", rf_grid.best_score_)

# 2. DecisionTreeClassifier
dt_params = {
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_params, cv=5, scoring='accuracy', n_jobs=-1)
dt_grid.fit(x_train, y_train)

print("Best Decision Tree Params:", dt_grid.best_params_)
print("Best Decision Tree Score:", dt_grid.best_score_)

# 3. SVC
svc_params = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.001],
    'kernel': ['rbf']
}

svc_grid = GridSearchCV(SVC(probability=True), svc_params, cv=5, scoring='accuracy', n_jobs=-1)
svc_grid.fit(x_train, y_train)

print("Best SVC Params:", svc_grid.best_params_)
print("Best SVC Score:", svc_grid.best_score_)

best_rf_model = rf_grid.best_estimator_
best_rf_model.predict(x_test)
'''

df['WinnerBinary'].value_counts(normalize=True)

'''

# 2. Define hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],         # Number of trees
    'max_depth': [5, 10, 20, None],          # Tree depth
    'min_samples_split': [2, 5, 10],         # Min samples to split a node
    'min_samples_leaf': [1, 2, 4],           # Min samples at a leaf node
    'max_features': ['sqrt', 'log2']         # Number of features to consider
}

# 3. Initialize base model
rf = RandomForestClassifier(random_state=42)

# 4. Grid search with 5-fold cross-validation
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# 5. Fit the grid search to the training data
grid_search.fit(x_train, y_train)

# 6. Evaluate best model
best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(x_test)

# 7. Report results
print(" Best Parameters:", grid_search.best_params_)
print(" Tuned Random Forest Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
'''